(window.webpackJsonp=window.webpackJsonp||[]).push([[26],{126:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return r})),n.d(t,"metadata",(function(){return s})),n.d(t,"rightToc",(function(){return c})),n.d(t,"default",(function(){return d}));var a=n(1),o=n(6),i=(n(0),n(141)),r={title:"JavaScript"},s={id:"primers/javascript",title:"JavaScript",description:"This article is a general introduction to the Web Audio API aimed at web",source:"@site/docs/primers/javascript.md",permalink:"/docs/primers/javascript",sidebar:"main",previous:{title:"Music",permalink:"/docs/primers/music"},next:{title:"Notes",permalink:"/docs/music/notes"}},c=[{value:"Background",id:"background",children:[]},{value:"The Web Audio API",id:"the-web-audio-api",children:[]},{value:"Graphs &amp; Nodes",id:"graphs--nodes",children:[]},{value:"Nodes",id:"nodes",children:[{value:"Source Nodes",id:"source-nodes",children:[]},{value:"Effect Nodes",id:"effect-nodes",children:[]}]},{value:"Timing Model",id:"timing-model",children:[]},{value:"Aside: Autoplay Policy",id:"aside-autoplay-policy",children:[]},{value:"Conclusion",id:"conclusion",children:[]},{value:"Further Reading",id:"further-reading",children:[]}],l={rightToc:c};function d(e){var t=e.components,n=Object(o.a)(e,["components"]);return Object(i.b)("wrapper",Object(a.a)({},l,n,{components:t,mdxType:"MDXLayout"}),Object(i.b)("p",null,"This article is a general introduction to the Web Audio API aimed at web\ndevelopers who are interested in extending their skills to work with audio and\nmusic applications."),Object(i.b)("p",null,"We start by defining what the Web Audio API is and where it came from, then move\non to cover two of its key concepts that may be unfamiliar to web developers:\ngraphs and timing. Along the way we'll get a feel for the API and what it can\ndo."),Object(i.b)("h2",{id:"background"},"Background"),Object(i.b)("p",null,"The Web Audio API is a set of APIs for generating and processing audio in the\nbrowser. It's designed by the\n",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://www.w3.org/2011/audio/"}),"W3C Audio Working Group"),', steered by Google,\nMozilla, and the BBC, and chartered to "add advanced sound and music synthesis\ncapabilities to the Open Web Platform."'),Object(i.b)("p",null,"It has been in development\n",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://www.w3.org/TR/2011/WD-webaudio-20111215/"}),"since 2011"),", with the spec\nrecently reaching v1.0 (with\n",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://github.com/WebAudio/web-audio-api-v2"}),"v2.0 in the works"),") and becoming\na ",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://www.w3.org/TR/webaudio/"}),"W3C Candidate Recomendation"),". Despite its\ncandiditate status, it's already well supported in browsers,\n",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://caniuse.com/#feat=audio-api"}),"reaching 94% of users")," at time of writing."),Object(i.b)("p",null,"Common use-cases cited for the Web Audio API are to bring music composition and\naudio editing tools (e.g. Ableton Live, Logic Pro) to the browser, as well as\nenabling real-time audio for games and VR. Beyond these obvious applications,\nbringing sound to the mix opens up another dimension for building a richer, more\nsensory web."),Object(i.b)("h2",{id:"the-web-audio-api"},"The Web Audio API"),Object(i.b)("p",null,"The Web Audio API itself is relatively small, and is covered comprehensively by\nthe\n",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API"}),"MDN Web Docs"),",\nthe go-to source for understanding everything the API can do."),Object(i.b)("p",null,"Rather than repeat the documentation here, we'll instead focus on the two\naspects of working with the API that are most different from traditional web\ndevelopment, namely the audio graph and the timing model. We'll cover just\nenough of the API to get up and running creating our own bleeps and bloops."),Object(i.b)("h2",{id:"graphs--nodes"},"Graphs & Nodes"),Object(i.b)("p",null,"As web developers, we're used to working with the DOM, a tree data structure\nrepresenting a hierarchy of nodes that we traverse via parent, sibling and child\nrelationships. The Web Audio API, on the other hand (and audio apps in general),\norganizes nodes in a ",Object(i.b)("em",{parentName:"p"},"graph")," data structure."),Object(i.b)("p",null,"[ILLUSTRATION]"),Object(i.b)("p",null,Object(i.b)("em",{parentName:"p"},"Source")," nodes generate signals and are the inputs of our system. These signals\nare routed through ",Object(i.b)("em",{parentName:"p"},"effect")," nodes to modify them in various ways. Everything\nends up at a single ",Object(i.b)("em",{parentName:"p"},"destination")," node, i.e. your speakers, producing the\naudible output of our system. This is digital signal processing 101."),Object(i.b)("p",null,"It's worth taking a moment to understand the fundamental differences between\ntree and graph data structures.\n",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://en.wikipedia.org/wiki/Graph_(abstract_data_type)"}),"Graph theory")," is\nit's own rich topic, but a key point to note is that a graph is not a heirarchy,\nbut is instead like a flow chart or electrical circuit. Each node is 'equal' and\ncan be connected to any (and many) other nodes, and connections can be circular\n(in fact this is essential to produce certain types of effects)."),Object(i.b)("p",null,"If we think about it, this is not such a new concept. We build pages (nodes)\nwhich we link together to form websites and apps (graphs), which together form\nthe internet, itself a graph of servers, routers, etc."),Object(i.b)("p",null,"You might come across graphs described in more technical terms. Graph nodes are\nalso know as ",Object(i.b)("em",{parentName:"p"},"vertices"),", with the connections between them known as ",Object(i.b)("em",{parentName:"p"},"edges"),".\nTraversing a graph is done by following its edges. A graph data structure\ngenerally has ways to find which vertices are connected (either directly or via\nother vertices), and insert and remove vertices and edges at given points in the\ngraph. Specifically, the Web Audio API uses a ",Object(i.b)("em",{parentName:"p"},"directed graph"),", that is, the\nsignals flow in a defined direction."),Object(i.b)("p",null,"A big part of working with the Web Audio API involves creating nodes and making\nsure they are wired up correctly in our graph. So let's see how this works..."),Object(i.b)("p",null,"###\xa0Creating Our Graph"),Object(i.b)("p",null,"Our graph exists in an ",Object(i.b)("inlineCode",{parentName:"p"},"AudioContext"),". Everything we do via the Web Audio API\nhappens in this context, similar to how a ",Object(i.b)("inlineCode",{parentName:"p"},"<canvas>")," element creates its own\nenvironment for drawing. We'll generally only create a single ",Object(i.b)("inlineCode",{parentName:"p"},"AudioContext")," per\napp and use its factory methods to create the nodes of our graph:"),Object(i.b)("pre",null,Object(i.b)("code",Object(a.a)({parentName:"pre"},{className:"language-js"}),"const context = new AudioContext()\nlet osc = context.createOscillator()\nlet vol = context.createGain()\nosc.connect(vol)\nvol.connect(context.destination)\n")),Object(i.b)("p",null,"Here we create an ",Object(i.b)("inlineCode",{parentName:"p"},"AudioContext"),", and from it create two different node types.\nan ",Object(i.b)("inlineCode",{parentName:"p"},"OscillatorNode"),", a type of ",Object(i.b)("em",{parentName:"p"},"source")," node, and a ",Object(i.b)("inlineCode",{parentName:"p"},"GainNode"),", a type of\n",Object(i.b)("em",{parentName:"p"},"effect")," node (we'll cover what these nodes actually do shortly). We connect the\n",Object(i.b)("inlineCode",{parentName:"p"},"OscillatorNode")," to the ",Object(i.b)("inlineCode",{parentName:"p"},"GainNode"),", and the ",Object(i.b)("inlineCode",{parentName:"p"},"GainNode")," to the ",Object(i.b)("em",{parentName:"p"},"destination")," node\nof the ",Object(i.b)("inlineCode",{parentName:"p"},"AudioContext")," i.e. our speakers. Our graph looks as follows (rendered in\nChrome with the ",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://google.github.io/audion/"}),"Web Audio Inspector"),")."),Object(i.b)("p",null,Object(i.b)("img",Object(a.a)({parentName:"p"},{src:"/img/primers/javascript/basic-graph.png",alt:null}))),Object(i.b)("h2",{id:"nodes"},"Nodes"),Object(i.b)("p",null,"Everything we create in our graph is a node. All nodes implement the\n",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://developer.mozilla.org/en-US/docs/Web/API/AudioNode"}),Object(i.b)("inlineCode",{parentName:"a"},"AudioNode")),"\ninterface, with additional properties and methods specific to their type."),Object(i.b)("p",null,"A node's properties can be get and set as you'd expect, but with an additional\nsuper-power: they implement the\n",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://developer.mozilla.org/en-US/docs/Web/API/AudioParam"}),Object(i.b)("inlineCode",{parentName:"a"},"AudioParam")),"\ninterface, meaning changes to them can be scheduled over time:"),Object(i.b)("pre",null,Object(i.b)("code",Object(a.a)({parentName:"pre"},{className:"language-js"}),"const context = new AudioContext()\nlet osc = context.createOscillator()\n\n// Set frequency now\nosc.frequency = 440\n\n// Change frequency in 1 second\noscillator.frequency.setValueAtTime(880, context.currentTime + 1)\n")),Object(i.b)("p",null,"Nodes all implement the ",Object(i.b)("inlineCode",{parentName:"p"},"connect()")," method, which is how you connect the output\nof one node to the input of others. This chaining is what creates our graph."),Object(i.b)("p",null,"Nodes themselves can be grouped into two types: Source Nodes and Effect Nodes."),Object(i.b)("h3",{id:"source-nodes"},"Source Nodes"),Object(i.b)("p",null,"Source nodes are anything that produce an audio signal and are the inputs of our\nsystem. There are several types of source node, but we'll cover just the two\nmost common here:"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},Object(i.b)("a",Object(a.a)({parentName:"li"},{href:"https://developer.mozilla.org/en-US/docs/Web/API/OscillatorNode"}),Object(i.b)("inlineCode",{parentName:"a"},"OscillatorNode")),"\n\u2013\xa0for synthesizing our own sounds."),Object(i.b)("li",{parentName:"ul"},Object(i.b)("a",Object(a.a)({parentName:"li"},{href:"https://developer.mozilla.org/en-US/docs/Web/API/AudioBufferSourceNode"}),Object(i.b)("inlineCode",{parentName:"a"},"AudioBufferSourceNode")),"\n\u2013 for playing back recorded sounds (samples).")),Object(i.b)("p",null,"All source nodes have a ",Object(i.b)("inlineCode",{parentName:"p"},"start()")," and ",Object(i.b)("inlineCode",{parentName:"p"},"stop()")," method, which as you might guess,\nstart and stop them producing their audio signal."),Object(i.b)("p",null,Object(i.b)("strong",{parentName:"p"},"Note:")," Source nodes are single use only, or 'fire and forget'. Once a node\nhas stopped (either by manually calling ",Object(i.b)("inlineCode",{parentName:"p"},"stop()"),", or by reaching the end of the\nsample it was playing), it cannot be restarted. In creating a piano instrument,\nour instinct might be to create 88 nodes, one for each key. Instead, we actually\nneed to create a new node each time a key is pressed. In this way, our audio\ngraph is not something fixed that we define ahead of time, but is instead a\ndynamic structure that changes as new nodes are created and discarded. Source\nnodes are intentionally cheap to create and stopped nodes are automatically\ngarbage-collected for us."),Object(i.b)("h4",{id:"oscillatornode"},"OscillatorNode"),Object(i.b)("p",null,"To synthesize our own sounds, we'd use an ",Object(i.b)("inlineCode",{parentName:"p"},"OscillatorNode"),". This produces a\nwaveform (sine, square, triangle, etc.) oscillating at a given frequency\n(specified in hertz). By combining different types of oscillators and effects,\nwe can produce an infinite range of sounds."),Object(i.b)("pre",null,Object(i.b)("code",Object(a.a)({parentName:"pre"},{className:"language-js"}),"const context = new AudioContext()\n\nconst bass = context.createOscillator()\nbass.type = 'sine'\nbass.frequency = 220\nbass.frequency.linearRampToValueAtTime(880, context.currentTime + 2)\n\nconst hi = context.createOscillator()\nhi.type = 'square'\nhi.frequency = 660\nhi.frequency.linearRampToValueAtTime(880, context.currentTime + 6)\n\nbass.connect(context.destination)\nhi.connect(context.destination)\n\nbass.start()\nhi.start()\n")),Object(i.b)("h4",{id:"audiobuffersourcenode"},"AudioBufferSourceNode"),Object(i.b)("p",null,"To play back recorded sounds (samples), we'd use an ",Object(i.b)("inlineCode",{parentName:"p"},"AudioBufferSourceNode"),".\nThis node is responsible for playing back and controlling the sample, but the\nsample itself is stored in an ",Object(i.b)("inlineCode",{parentName:"p"},"AudioBuffer"),". In this way we can load a sample\nonce, and use it many times."),Object(i.b)("p",null,"Loading a sample is a two step process. We first need to fetch it over the\nnetwork, then decode it into a format that ",Object(i.b)("inlineCode",{parentName:"p"},"AudioBuffer")," understands."),Object(i.b)("p",null,"The whole process of loading, decoding, and finally playing back a sample looks\nas follows:"),Object(i.b)("pre",null,Object(i.b)("code",Object(a.a)({parentName:"pre"},{className:"language-js"}),"const context = new AudioContext()\n\nconst response = await fetch('http://example.com/meow.mp3')\nconst arrayBuffer = await response.arrayBuffer()\nconst audioBuffer = await context.decodeAudioData(arrayBuffer)\n\nconst sourceNode = context.createBufferSource()\nsourceNode.buffer = audioBuffer\nsourceNode.connect(context.destination)\nsourceNode.start()\n")),Object(i.b)("h3",{id:"effect-nodes"},"Effect Nodes"),Object(i.b)("p",null,"We can route an audio signal (coming from a source node) through a wide range of\neffect nodes. These modify the incoming signal in some way, producing a new\nsignal as output. The Web Audio API defines some basic primitives, which can be\ncombined to create all sorts of effects. The most common ones are:"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},Object(i.b)("a",Object(a.a)({parentName:"li"},{href:"https://developer.mozilla.org/en-US/docs/Web/API/GainNode"}),Object(i.b)("inlineCode",{parentName:"a"},"GainNode"))," \u2013\nadjusts the volume of a signal. By applying this over time we can also model\nADSR envelopes (e.g. if a sound starts abruptly or fades in slowly)."),Object(i.b)("li",{parentName:"ul"},Object(i.b)("a",Object(a.a)({parentName:"li"},{href:"https://developer.mozilla.org/en-US/docs/Web/API/BiquadFilterNode"}),Object(i.b)("inlineCode",{parentName:"a"},"BiquadFilterNode"))," -\ncut or boost certain frequencies of a signal"),Object(i.b)("li",{parentName:"ul"},"ConvolverNode - apply reverb to a signal so it sounds like it's in a certain\nphysical space e.g. a small room or large hall."),Object(i.b)("li",{parentName:"ul"},"DelayNode - apply a delay the the outgoing signal, used for all sorts of\neffects from echoes to phasing."),Object(i.b)("li",{parentName:"ul"},"DynamicsCompressorNode \u2013 applies compression to a signal to control its\ndynamic range (e.g. to avoid distortion)"),Object(i.b)("li",{parentName:"ul"},"WaveShaperNode \u2013 applies distortion to the signal."),Object(i.b)("li",{parentName:"ul"},"PannerNode \u2013 places the audio in the stereo field (i.e. to the left or right\nin stereo output).")),Object(i.b)("p",null,"###\xa0Destination Node"),Object(i.b)("p",null,"The detination node is the final node in the chain and represents our audio\noutput device (i.e. our sound card). This is provided for us as the\n",Object(i.b)("inlineCode",{parentName:"p"},"destination")," node of our AudioContext. In addition, there's also an\nofflineAudioContext with its own destination if we want to render our audio to\ndisk, for example."),Object(i.b)("h2",{id:"timing-model"},"Timing Model"),Object(i.b)("p",null,"To understand the timing model, we need to understand under the hood, control\nthread = your code, rendering thread = where audio is actually computed."),Object(i.b)("p",null,"Timing: has it's own internal clock. Separate from JS clock. JS clock can be\nthought of as lazy/relative/imprecise. Usually we'd schedule a callback in\n2000ms. JS will schedule this and run when it's done with other tasks, in\nroughly 2 seconds."),Object(i.b)("p",null,"Web Audio clock is absolute time. Precise. Seconds, not milliseconds! Relative\nto the current time."),Object(i.b)("p",null,"ILLUSTRATION?"),Object(i.b)("p",null,"Current time is defined by the AudioContext. It starts counting up from the\nmoment the AudioContext is created. We can't change it, we can only get it and\nschedule things relative to it:"),Object(i.b)("pre",null,Object(i.b)("code",Object(a.a)({parentName:"pre"},{className:"language-js"}),"const context = new AudioContext()\ncontext.currentTime // => 0.1234\n")),Object(i.b)("p",null,"As you can see, the Web Audio API doesn't give us much to work with here. We\nneed to invent our own timing abstraction to work with beats, bars, time\nsignatures etc. But that's for a future article!"),Object(i.b)("h2",{id:"aside-autoplay-policy"},"Aside: Autoplay Policy"),Object(i.b)("p",null,"Needs interaction. Needs some hijinx."),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"Chrome: ",Object(i.b)("inlineCode",{parentName:"li"},"chrome://flags/#autoplay-policy")," \u2013 no longer working as of v76"),Object(i.b)("li",{parentName:"ul"},"Firefox: enabled by default?"),Object(i.b)("li",{parentName:"ul"},"Safari: ?"),Object(i.b)("li",{parentName:"ul"},"Edge: ?")),Object(i.b)("p",null,"Can be allowed for specific domains:"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"Chrome: chrome://settings/content/sound")),Object(i.b)("p",null,"References:"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},Object(i.b)("a",Object(a.a)({parentName:"li"},{href:"https://developer.mozilla.org/en-US/docs/Web/Media/Autoplay_guide#Autoplay_using_the_Web_Audio_API"}),"https://developer.mozilla.org/en-US/docs/Web/Media/Autoplay_guide#Autoplay_using_the_Web_Audio_API")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("a",Object(a.a)({parentName:"li"},{href:"https://developers.google.com/web/updates/2017/09/autoplay-policy-changes#webaudio"}),"https://developers.google.com/web/updates/2017/09/autoplay-policy-changes#webaudio"))),Object(i.b)("h2",{id:"conclusion"},"Conclusion"),Object(i.b)("p",null,"The WAA is a well defined and adopted/supported technology that's actively\ndeveloped. It is and will continue to enable a whole range of previously\nnative-tethered apps to move to the web."),Object(i.b)("p",null,"The WAA provides only the basic primitives. Because it's use-cases are wide.\nDepending on what we want to do, we have to build our own abstractions. There\nare already a range of libraries for music, game audio, VR?, machine learning."),Object(i.b)("p",null,"If we understand two basic concepts: graph/nodes and timing model, we have a\ngood foundation for working at a higher level and make cool stuff."),Object(i.b)("h2",{id:"further-reading"},"Further Reading"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},Object(i.b)("a",Object(a.a)({parentName:"li"},{href:"https://teropa.info/blog/2016/07/28/javascript-systems-music.html"}),"JavaScript Systems Music")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("a",Object(a.a)({parentName:"li"},{href:"http://teropa.info/blog/2016/08/19/what-is-the-web-audio-api.html"}),"What Is the Web Audio API?")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("a",Object(a.a)({parentName:"li"},{href:"https://medium.com/@metalex9/making-generative-music-in-the-browser-bfb552a26b0b"}),"Making Generative Music in the Browser"))))}d.isMDXComponent=!0},141:function(e,t,n){"use strict";n.d(t,"a",(function(){return u})),n.d(t,"b",(function(){return h}));var a=n(0),o=n.n(a);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=o.a.createContext({}),d=function(e){var t=o.a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s({},t,{},e)),n},u=function(e){var t=d(e.components);return o.a.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return o.a.createElement(o.a.Fragment,{},t)}},b=Object(a.forwardRef)((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,r=e.parentName,l=c(e,["components","mdxType","originalType","parentName"]),u=d(n),b=a,h=u["".concat(r,".").concat(b)]||u[b]||p[b]||i;return n?o.a.createElement(h,s({ref:t},l,{components:n})):o.a.createElement(h,s({ref:t},l))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,r=new Array(i);r[0]=b;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:a,r[1]=s;for(var l=2;l<i;l++)r[l]=n[l];return o.a.createElement.apply(null,r)}return o.a.createElement.apply(null,n)}b.displayName="MDXCreateElement"}}]);